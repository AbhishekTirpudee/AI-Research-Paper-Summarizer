/**
 * JavaScript-based text compressor (replaces ScaleDown Python bridge).
 * 
 * Uses extractive compression â€” picks the most important sentences,
 * removes redundancy, and truncates to fit model input limits.
 * 
 * No Python dependency, no external API needed. Runs instantly.
 */

/**
 * Simple token estimation (~4 chars per token for English text)
 */
function estimateTokens(text) {
    if (!text) return 0;
    return Math.ceil(text.length / 4);
}

/**
 * Compress text by extracting the most important sentences.
 * Achieves ~40-60% reduction while preserving key information.
 * 
 * @param {string} context - The text to compress
 * @param {string} prompt - The query/task (used to guide extraction)
 * @param {number} maxTokens - Target maximum tokens (default: 1000)
 * @returns {Object} - {content, originalTokens, compressedTokens, compressionRatio}
 */
function compressText(context, prompt = '', maxTokens = 1000) {
    if (!context || context.trim().length < 100) {
        return {
            content: context || '',
            originalTokens: estimateTokens(context),
            compressedTokens: estimateTokens(context),
            compressionRatio: 1,
        };
    }

    const originalTokens = estimateTokens(context);
    const maxChars = maxTokens * 4; // ~4 chars per token

    // Split into sentences
    const sentences = context.match(/[^.!?\n]+[.!?\n]+/g) || [context];

    // Extract keywords from prompt for relevance scoring
    const promptKeywords = (prompt || '').toLowerCase().split(/\s+/).filter(w => w.length > 3);

    // Score each sentence
    const scored = sentences.map((sentence, index) => {
        const trimmed = sentence.trim();
        if (trimmed.length < 10) return { text: trimmed, score: 0, index };

        let score = 0;

        // Position bonus: first and last sentences are often most important
        if (index < 3) score += 3 - index;
        if (index >= sentences.length - 2) score += 1.5;

        // Length bonus: medium-length sentences are usually more informative
        const wordCount = trimmed.split(/\s+/).length;
        if (wordCount >= 10 && wordCount <= 40) score += 2;
        else if (wordCount >= 5) score += 1;

        // Keyword bonus: sentences containing important academic terms
        const lower = trimmed.toLowerCase();
        const importantTerms = [
            'result', 'finding', 'conclude', 'demonstrate', 'significant',
            'propose', 'method', 'approach', 'contribute', 'novel',
            'improve', 'outperform', 'achieve', 'show', 'suggest',
            'hypothesis', 'experiment', 'evaluate', 'analysis', 'model',
        ];
        for (const term of importantTerms) {
            if (lower.includes(term)) score += 1.5;
        }

        // Prompt relevance bonus
        for (const kw of promptKeywords) {
            if (lower.includes(kw)) score += 2;
        }

        // Penalize very short or repetitive sentences
        if (wordCount < 5) score -= 2;

        return { text: trimmed, score, index };
    });

    // Sort by score (highest first), pick top sentences
    scored.sort((a, b) => b.score - a.score);

    let result = '';
    const selected = [];

    for (const s of scored) {
        if (s.score <= 0) continue;
        if ((result.length + s.text.length) > maxChars) break;
        selected.push(s);
        result += s.text + ' ';
    }

    // Re-sort selected sentences by original position (preserve flow)
    selected.sort((a, b) => a.index - b.index);
    const compressed = selected.map(s => s.text).join(' ').trim();

    // If we didn't extract enough, just truncate the original
    const finalContent = compressed.length > 50 ? compressed : context.substring(0, maxChars);
    const compressedTokens = estimateTokens(finalContent);

    return {
        content: finalContent,
        originalTokens,
        compressedTokens,
        compressionRatio: compressedTokens > 0 ? Math.round((originalTokens / compressedTokens) * 100) / 100 : 1,
    };
}

module.exports = { compressText, estimateTokens };
